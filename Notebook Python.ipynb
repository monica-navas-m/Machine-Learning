{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "310cddea",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING GROUP PROJECT \n",
    "\n",
    "### Group Members: Helena Krumm (55577),Marouan Kamoun (53833), Mila Gardini (54742), Monica Navas (54577), Yassine Hafi (54466) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e9e61",
   "metadata": {},
   "source": [
    "## **Index**\n",
    "* [Overview and Business Applications](#intro)\n",
    "* [Dataset Information (Metadata)](#meta)\n",
    "* [Import libraries](#libraries)\n",
    "* [Reading and Data Cleaning](#clean)\n",
    "* [Exploratory Data Analysis](#eda)\n",
    "* [Model 1 : Decision Tree ](#model1)\n",
    "* [Model 2 : Logistic Regression ](#model2)\n",
    "* [Model 3 : ANN ](#model3)\n",
    "* [Model 4 : Random Forests ](#model4)\n",
    "* [Model 5 : XG Boosting ](#model5)\n",
    "* [Model 6 : Ensemble Model ](#model5)\n",
    "* [Model Comparison](#compare)\n",
    "* [Best Model Selection and Improvement](#best)\n",
    "* [Conclusion](#conclusion)\n",
    "* [Sources](#sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa4690",
   "metadata": {},
   "source": [
    "<a name=\"intro\"></a>\n",
    "## Overview and Business applications\n",
    "\n",
    "Forests are one of the most critical ecosystems on our planet, and they provide a range of benefits to both people and the environment. However, forests around the world are facing numerous threats, including deforestation, invasive species, and climate change impacts. To address these issues, we need a better understanding of the state of the forest ecosystem and the factors that influence forest health. This is where environmental monitoring comes in.\n",
    "\n",
    "Environmental monitoring is a critical aspect of sustainable forest management, as it allows us to understand the state of the forest ecosystem and identify potential threats to its health. By analyzing the cartographic variables and forest cover type data in this project, we can gain insights into the relationship between different environmental factors and forest health. This information can be used to develop more effective environmental monitoring programs that can help us identify and address issues such as deforestation, invasive species, and climate change impacts. The application of this project lies in the ability to use machine learning models to analyze the environmental variables and make predictions about forest health. Businesses and organizations in the forestry industry can use these predictions to make better decisions about forest management practices, such as where to focus conservation efforts or how to plan for potential climate change impacts. In addition, this information can be used to inform policy decisions around sustainable forest management and conservation.\n",
    "\n",
    "The study area includes four wilderness areas located in the Roosevelt National Forest of northern Colorado. These areas represent forests with minimal human-caused disturbances, so that existing forest cover types are more a result of ecological processes rather than forest management practices. By analyzing this data set using machine learning models, we can gain insights into the relationship between these cartographic variables and forest health. This, in turn, can help us develop more effective environmental monitoring programs that can identify and address issues such as deforestation, invasive species, and climate change impacts.\n",
    "\n",
    "Overall, this project has significant implications for environmental monitoring and sustainable forest management. By using machine learning to analyze cartographic variables and forest cover type data, we can gain valuable insights into the state of the forest ecosystem and develop more effective environmental monitoring programs. This, in turn, can help us address critical issues facing our forests and ensure their long-term health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500cf4e",
   "metadata": {},
   "source": [
    "<a name=\"meta\"></a>\n",
    "## Dataset informations\n",
    "\n",
    "Number of instances (observations):  581,012\n",
    "\n",
    "Number of Attributes:\t12 measures, but 54 columns of data(10 quantitative variables, 4 binary wilderness areas and 40 binary soil type variables)\n",
    "\n",
    "\n",
    "1.\tAttribute information:\n",
    "\n",
    "Given is the attribute name, attribute type, the measurement unit and a brief description.  The forest cover type is the classification problem. \n",
    "\n",
    "    Name                                     Data Type    Measurement                       Description\n",
    "\n",
    "    Elevation                               quantitative    meters                       Elevation in meters\n",
    "    Aspect                                  quantitative    azimuth                      Aspect in degrees azimuth\n",
    "    Slope                                   quantitative    degrees                      Slope in degrees\n",
    "    Horizontal_Distance_To_Hydrology        quantitative    meters                       Horz Dist to nearest surface water features\n",
    "    Vertical_Distance_To_Hydrology          quantitative    meters                       Vert Dist to nearest surface water features\n",
    "    Horizontal_Distance_To_Roadways         quantitative    meters                       Horz Dist to nearest roadway\n",
    "    Hillshade_9am                           quantitative    0 to 255 index               Hillshade index at 9am, summer solstice\n",
    "    Hillshade_Noon                          quantitative    0 to 255 index               Hillshade index at noon, summer soltice\n",
    "    Hillshade_3pm                           quantitative    0 to 255 index               Hillshade index at 3pm, summer solstice\n",
    "    Horizontal_Distance_To_Fire_Points      quantitative    meters                       Horz Dist to nearest wildfire ignition points\n",
    "    Wilderness_Area (4 binary columns)      qualitative     0 (absence) or 1 (presence)  Wilderness area designation\n",
    "    Soil_Type (40 binary columns)           qualitative     0 (absence) or 1 (presence)  Soil Type designation\n",
    "    Cover_Type (7 types)                    integer         1 to 7                       Forest Cover Type designation\n",
    "\n",
    "\n",
    "Code Designations:\n",
    "\n",
    "    Wilderness Areas:  \t1 -- Rawah Wilderness Area\n",
    "                        2 -- Neota Wilderness Area\n",
    "                        3 -- Comanche Peak Wilderness Area\n",
    "                        4 -- Cache la Poudre Wilderness Area\n",
    "\n",
    "Soil Types:             1 to 40 : based on the USFS Ecological\n",
    "                        Landtype Units (ELUs) for this study area:\n",
    "\n",
    "\n",
    "      Forest Cover Type Classes:\t1 -- Spruce/Fir\n",
    "                                    2 -- Lodgepole Pine\n",
    "                                    3 -- Ponderosa Pine\n",
    "                                    4 -- Cottonwood/Willow\n",
    "                                    5 -- Aspen\n",
    "                                    6 -- Douglas-fir\n",
    "                                    7 -- Krummholz\n",
    "\n",
    "\n",
    "2.  Basic Summary Statistics for quantitative variables only\n",
    "\t(whole dataset -- thanks to Phil Rennert for the summary values):\n",
    "\n",
    "            Name                                    Units             Mean   Std Dev\n",
    "            Elevation                               meters          2959.36  279.98\n",
    "            Aspect                                  azimuth          155.65  111.91\n",
    "            Slope                                   degrees           14.10    7.49\n",
    "            Horizontal_Distance_To_Hydrology        meters           269.43  212.55\n",
    "            Vertical_Distance_To_Hydrology          meters            46.42   58.30\n",
    "            Horizontal_Distance_To_Roadways         meters          2350.15 1559.25\n",
    "            Hillshade_9am                           0 to 255 index   212.15   26.77\n",
    "            Hillshade_Noon                          0 to 255 index   223.32   19.77\n",
    "            Hillshade_3pm                           0 to 255 index   142.53   38.27\n",
    "            Horizontal_Distance_To_Fire_Points      meters          1980.29 1324.19\n",
    "\n",
    "\n",
    "3.\tMissing Attribute Values:  None.\n",
    "\n",
    "\n",
    "4.\tClass distribution:\n",
    "\n",
    "           Number of records of Spruce-Fir:                211840 \n",
    "           Number of records of Lodgepole Pine:            283301 \n",
    "           Number of records of Ponderosa Pine:             35754 \n",
    "           Number of records of Cottonwood/Willow:           2747\n",
    "           Number of records of Aspen:                       9493 \n",
    "           Number of records of Douglas-fir:                17367 \n",
    "           Number of records of Krummholz:                  20510  \n",
    "           Number of records of other:                          0  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a47e565",
   "metadata": {},
   "source": [
    "<a name=\"libraries\"></a>\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c1df99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, classification_report, mean_squared_error, plot_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c29900",
   "metadata": {},
   "source": [
    "<a name=\"clean\"></a>\n",
    "## Data Loading and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6e434b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>Cover_Type_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2579</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>-15</td>\n",
       "      <td>67</td>\n",
       "      <td>230</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>6031</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0       2590      56      2                               212   \n",
       "1       2804     139      9                               268   \n",
       "2       2785     155     18                               242   \n",
       "3       2595      45      2                               153   \n",
       "4       2579     132      6                               300   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology   Horizontal_Distance_To_Roadways  \\\n",
       "0                               -6                              390   \n",
       "1                               65                             3180   \n",
       "2                              118                             3090   \n",
       "3                               -1                              391   \n",
       "4                              -15                               67   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0            220             235            151   \n",
       "1            234             238            135   \n",
       "2            238             238            122   \n",
       "3            220             234            150   \n",
       "4            230             237            140   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points  ...  32  33  34  35  36  37  38  39  \\\n",
       "0                                6225  ...   0   0   0   0   0   0   0   0   \n",
       "1                                6121  ...   0   0   0   0   0   0   0   0   \n",
       "2                                6211  ...   0   0   0   0   0   0   0   0   \n",
       "3                                6172  ...   0   0   0   0   0   0   0   0   \n",
       "4                                6031  ...   0   0   0   0   0   0   0   0   \n",
       "\n",
       "   40  Cover_Type_code  \n",
       "0   0                5  \n",
       "1   0                2  \n",
       "2   0                2  \n",
       "3   0                5  \n",
       "4   0                2  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"./dataset/covtype_data.csv\")\n",
    "\n",
    "# Define column names\n",
    "col_names = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology ',\n",
    "             'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "             'Horizontal_Distance_To_Fire_Points']\n",
    "\n",
    "# Generate names for binary columns based on their position\n",
    "binary_col_names = [i+1 for i in range(4)]\n",
    "binary_col_names += [i+1 for i in range(40)]\n",
    "\n",
    "# Append the binary column names to the list of column names\n",
    "col_names += binary_col_names\n",
    "\n",
    "# Append the target variable name to the list of column names\n",
    "col_names.append('Cover_Type_code')\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "df.columns = col_names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf08c065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Cover_Type_code</th>\n",
       "      <th>Soil type code</th>\n",
       "      <th>Wilderness area code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2579</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>-15</td>\n",
       "      <td>67</td>\n",
       "      <td>230</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>6031</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0       2590      56      2                               212   \n",
       "1       2804     139      9                               268   \n",
       "2       2785     155     18                               242   \n",
       "3       2595      45      2                               153   \n",
       "4       2579     132      6                               300   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology   Horizontal_Distance_To_Roadways  \\\n",
       "0                               -6                              390   \n",
       "1                               65                             3180   \n",
       "2                              118                             3090   \n",
       "3                               -1                              391   \n",
       "4                              -15                               67   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0            220             235            151   \n",
       "1            234             238            135   \n",
       "2            238             238            122   \n",
       "3            220             234            150   \n",
       "4            230             237            140   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points  Cover_Type_code  Soil type code  \\\n",
       "0                                6225                5              29   \n",
       "1                                6121                2              12   \n",
       "2                                6211                2              30   \n",
       "3                                6172                5              29   \n",
       "4                                6031                2              29   \n",
       "\n",
       "   Wilderness area code  \n",
       "0                     1  \n",
       "1                     1  \n",
       "2                     1  \n",
       "3                     1  \n",
       "4                     1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Grouping the soil type columns into 1 column\n",
    "df['Soil type code'] = df.iloc[:, 14:54].idxmax(axis=1)\n",
    "df = pd.concat([df.iloc[:, :14], df.iloc[:, 54:]], axis=1)\n",
    "#Grouping the Wilderness Areas columns into 1 column\n",
    "df['Wilderness area code'] = df.iloc[:, 10:14].idxmax(axis=1)\n",
    "df = pd.concat([df.iloc[:, :10], df.iloc[:, 14:]], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb624cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Horizontal_Distance_To_Hydrology</th>\n",
       "      <th>Vertical_Distance_To_Hydrology</th>\n",
       "      <th>Horizontal_Distance_To_Roadways</th>\n",
       "      <th>Hillshade_9am</th>\n",
       "      <th>Hillshade_Noon</th>\n",
       "      <th>Hillshade_3pm</th>\n",
       "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
       "      <th>Soil type code</th>\n",
       "      <th>Wilderness area code</th>\n",
       "      <th>Wilderness area description</th>\n",
       "      <th>soil type description</th>\n",
       "      <th>Cover_Type_code</th>\n",
       "      <th>cover type description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2590</td>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>212</td>\n",
       "      <td>-6</td>\n",
       "      <td>390</td>\n",
       "      <td>220</td>\n",
       "      <td>235</td>\n",
       "      <td>151</td>\n",
       "      <td>6225</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>Rawah Wilderness Area</td>\n",
       "      <td>Como - Legault families complex, extremely stony.</td>\n",
       "      <td>5</td>\n",
       "      <td>Aspen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2804</td>\n",
       "      <td>139</td>\n",
       "      <td>9</td>\n",
       "      <td>268</td>\n",
       "      <td>65</td>\n",
       "      <td>3180</td>\n",
       "      <td>234</td>\n",
       "      <td>238</td>\n",
       "      <td>135</td>\n",
       "      <td>6121</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Rawah Wilderness Area</td>\n",
       "      <td>Legault family - Rock land complex, stony.</td>\n",
       "      <td>2</td>\n",
       "      <td>Lodgepole Pine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2785</td>\n",
       "      <td>155</td>\n",
       "      <td>18</td>\n",
       "      <td>242</td>\n",
       "      <td>118</td>\n",
       "      <td>3090</td>\n",
       "      <td>238</td>\n",
       "      <td>238</td>\n",
       "      <td>122</td>\n",
       "      <td>6211</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>Rawah Wilderness Area</td>\n",
       "      <td>Como family - Rock land - Legault family compl...</td>\n",
       "      <td>2</td>\n",
       "      <td>Lodgepole Pine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2595</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>391</td>\n",
       "      <td>220</td>\n",
       "      <td>234</td>\n",
       "      <td>150</td>\n",
       "      <td>6172</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>Rawah Wilderness Area</td>\n",
       "      <td>Como - Legault families complex, extremely stony.</td>\n",
       "      <td>5</td>\n",
       "      <td>Aspen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2579</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>-15</td>\n",
       "      <td>67</td>\n",
       "      <td>230</td>\n",
       "      <td>237</td>\n",
       "      <td>140</td>\n",
       "      <td>6031</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>Rawah Wilderness Area</td>\n",
       "      <td>Como - Legault families complex, extremely stony.</td>\n",
       "      <td>2</td>\n",
       "      <td>Lodgepole Pine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
       "0       2590      56      2                               212   \n",
       "1       2804     139      9                               268   \n",
       "2       2785     155     18                               242   \n",
       "3       2595      45      2                               153   \n",
       "4       2579     132      6                               300   \n",
       "\n",
       "   Vertical_Distance_To_Hydrology   Horizontal_Distance_To_Roadways  \\\n",
       "0                               -6                              390   \n",
       "1                               65                             3180   \n",
       "2                              118                             3090   \n",
       "3                               -1                              391   \n",
       "4                              -15                               67   \n",
       "\n",
       "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
       "0            220             235            151   \n",
       "1            234             238            135   \n",
       "2            238             238            122   \n",
       "3            220             234            150   \n",
       "4            230             237            140   \n",
       "\n",
       "   Horizontal_Distance_To_Fire_Points  Soil type code  Wilderness area code  \\\n",
       "0                                6225              29                     1   \n",
       "1                                6121              12                     1   \n",
       "2                                6211              30                     1   \n",
       "3                                6172              29                     1   \n",
       "4                                6031              29                     1   \n",
       "\n",
       "  Wilderness area description  \\\n",
       "0       Rawah Wilderness Area   \n",
       "1       Rawah Wilderness Area   \n",
       "2       Rawah Wilderness Area   \n",
       "3       Rawah Wilderness Area   \n",
       "4       Rawah Wilderness Area   \n",
       "\n",
       "                               soil type description  Cover_Type_code  \\\n",
       "0  Como - Legault families complex, extremely stony.                5   \n",
       "1         Legault family - Rock land complex, stony.                2   \n",
       "2  Como family - Rock land - Legault family compl...                2   \n",
       "3  Como - Legault families complex, extremely stony.                5   \n",
       "4  Como - Legault families complex, extremely stony.                2   \n",
       "\n",
       "  cover type description  \n",
       "0                  Aspen  \n",
       "1         Lodgepole Pine  \n",
       "2         Lodgepole Pine  \n",
       "3                  Aspen  \n",
       "4         Lodgepole Pine  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=df.copy()\n",
    "#add a column with the description of codes for Soil type and wilderness areas\n",
    "Wilderness_areas_list=['Rawah Wilderness Area','Neota Wilderness Area','Comanche Peak Wilderness Area' ,'Cache la Poudre Wilderness Area']\n",
    "df1['Wilderness area description']=[Wilderness_areas_list[col-1] for col in df1['Wilderness area code']]\n",
    "\n",
    "list_of_soil_type=pd.read_csv(\"./dataset/list_of_soil_types.csv\")\n",
    "list_of_soil_type=list(list_of_soil_type)\n",
    "df1['soil type description']=[list_of_soil_type[col-1] for col in df1['Soil type code']]\n",
    "\n",
    "new_order = [col for col in df1.columns if col != 'Cover_Type_code'] + ['Cover_Type_code']\n",
    "df1 = df1[new_order]\n",
    "\n",
    "cover_type_list=['Spruce/Fir','Lodgepole Pine','Ponderosa Pine','Cottonwood/Willow','Aspen','Douglas-fir','Krummholz']\n",
    "df1['cover type description']=[cover_type_list[col-1] for col in df1['Cover_Type_code']]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134a026e",
   "metadata": {},
   "source": [
    "<a name=\"eda\"></a>\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a1eb31",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis (EDA) is a critical step in the data analysis process that aims to summarize, visualize, and understand the underlying structure and patterns in a dataset. This helps in identifying trends, anomalies, and potential relationships among variables, which can ultimately lead to the development of appropriate statistical models and hypothesis testing.\n",
    "\n",
    "In the given project, the goal is to predict forest cover types based on cartographic variables, with data derived from the US Forest Service (USFS) and the US Geological Survey (USGS). The dataset consists of binary columns representing wilderness areas and soil types, as well as the target variable, which is the forest cover type.\n",
    "\n",
    "To perform the EDA, the team started by defining column names and generating names for binary columns based on their position. These binary column names were then appended to the list of column names, along with the target variable (cover type). After renaming the columns in the dataframe, the team proceeded to analyze the numerical columns by printing summary statistics. This helped to gain a better understanding of the central tendencies and dispersion within the data.\n",
    "\n",
    "To further explore the distribution of the target variable, we printed the count of each target value and created a histogram. This provided insights into the frequency of different forest cover types and highlighted potential imbalances in the dataset. By examining the dataset in this manner, our group was able to identify trends, patterns, and potential relationships among variables, which will help in selecting appropriate techniques for modeling and prediction in the next stages of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the summary statistics of the numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94874ea",
   "metadata": {},
   "source": [
    "The count, mean, min, and max rows are self-explanatory. The std row shows the standard deviation (which measures how dispersed the values are). The 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indicates the value below which a given percentage of observations in a group of observations falls.\n",
    "\n",
    "Another quick way to get a feel of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis). You can either plot this one attribute at a time, or you can call the hist() method on the whole dataset, and it will plot a histogram for each numerical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83cbd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.hist(bins=40, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d061d3",
   "metadata": {},
   "source": [
    "The histogram plot shows the distribution of all the numeric columns in the dataset.\n",
    "\n",
    "The \"Elevation\" column seems to be roughly normally distributed, with the majority of values falling in the range of 2500-3300.\n",
    "\n",
    "The \"Aspect\" column appears to have a roughly uniform distribution, with no clear trend or peak in the data.\n",
    "\n",
    "The \"Slope\" column has a right-skewed distribution, indicating that the majority of slopes are relatively gentle, with a long tail of steeper slopes.\n",
    "\n",
    "The \"Horizontal_Distance_To_Hydrology\" and \"Vertical_Distance_To_Hydrology\" columns both have a strong peak at 0, indicating that many of the observations have no distance to hydrology.\n",
    "\n",
    "The \"Horizontal_Distance_To_Roadways\" column is roughly normally distributed, with a peak around 0-500.\n",
    "\n",
    "The \"Hillshade\" columns (9am, Noon, and 3pm) are all roughly normally distributed, with peaks around 200-255.\n",
    "\n",
    "The \"Horizontal_Distance_To_Fire_Points\" column is right-skewed, indicating that the majority of observations have relatively short distances to fire points.\n",
    "\n",
    "Now we divide the data to train and test sets to explore the train set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b879f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df1, test_size=0.2, random_state=42)\n",
    "train = train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006aee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e381567",
   "metadata": {},
   "source": [
    "#### Distribution of cover type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac1c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the distribution'cover type description' column\n",
    "plt.figure(figsize=(10, 8))\n",
    "percentage = df1['Cover_Type_code'].value_counts(normalize=True) * 100\n",
    "# create a bar plot of the percentages of each unique value in the 'cover type description' column\n",
    "percentage.plot(kind='bar')\n",
    "# display the plot\n",
    "plt.xlabel('Cover Type')\n",
    "plt.ylabel('Percentage')\n",
    "\n",
    "# add percentage labels to the bars\n",
    "for index, value in enumerate(percentage):\n",
    "    code = percentage.index[index]\n",
    "    desc = df1[df1['Cover_Type_code'] == code]['cover type description'].unique()[0]\n",
    "    plt.text(index, value + 1, f'{code} - {desc}\\n{round(value, 2)}%', ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff05a0",
   "metadata": {},
   "source": [
    "#### Distribution of elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb484bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a density plot of the 'elevation' column\n",
    "sns.kdeplot(df1['Elevation'], shade=True)\n",
    "plt.xlabel('Elevation')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Elevation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cdc2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Cover_Type_code', y='Elevation', data=df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90fbcfc",
   "metadata": {},
   "source": [
    "From the plot, we can see that Cover_Type_code 1, 2, and 7 have relatively higher median elevations than the other categories. In contrast, Cover_Type_code 3 and 4 have lower median elevations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9727a",
   "metadata": {},
   "source": [
    "#### Distribution of Aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d799ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9fe004d",
   "metadata": {},
   "source": [
    "#### Distribution of wilderness area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af22ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the distribution'cover type description' column\n",
    "plt.figure(figsize=(10, 8))\n",
    "percentage = df1['Soil type code'].value_counts(normalize=True) * 100\n",
    "# create a bar plot of the percentages of each unique value in the 'Wilderness area description' column\n",
    "percentage.plot(kind='bar')\n",
    "# display the plot\n",
    "plt.xlabel('Wilderness area')\n",
    "plt.ylabel('percentage')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c15e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the distribution'cover type description' column\n",
    "plt.figure(figsize=(10, 8))\n",
    "percentage = df1['Wilderness area description'].value_counts(normalize=True) * 100\n",
    "# create a bar plot of the percentages of each unique value in the 'Wilderness area description' column\n",
    "percentage.plot(kind='bar')\n",
    "# display the plot\n",
    "plt.xlabel('Wilderness area')\n",
    "plt.ylabel('percentage')\n",
    "\n",
    "# add percentage labels to the bars\n",
    "for index, value in enumerate(percentage):\n",
    "    plt.text(index, value + 1, str(round(value, 2)) + '%', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0178f57d",
   "metadata": {},
   "source": [
    "# Models:\n",
    "\n",
    "In order to predict forest cover types based on cartographic variables, our team has selected three machine learning models, which are well-suited to this dataset: classification, decision tree, and clustering.\n",
    "\n",
    "Decision Tree: Decision trees are an ideal choice for our dataset because they can handle both numerical and categorical features, like the binary columns for wilderness areas and soil types. These models are easily interpretable and can capture non-linear relationships between features and the target variable. Moreover, they are capable of selecting the most relevant features for prediction, which can help improve overall model performance.\n",
    "\n",
    "Clustering: Although clustering models are typically used for unsupervised learning, they can be adapted to predict forest cover types by applying a semi-supervised approach. This involves using clustering algorithms, such as k-means or hierarchical clustering, to group similar data points in the feature space. Once clusters are formed, we can assign the majority class of each cluster as its representative forest cover type. This method can be particularly useful when the dataset exhibits clear patterns or groups that correspond to different cover types.\n",
    "\n",
    "Classification: We chose a classification model because our problem is a multiclass classification task, where the goal is to predict one of several possible forest cover types. Classification models, such as logistic regression, support vector machines (SVM), and k-nearest neighbors (KNN), can effectively predict categorical outcomes by learning patterns in the dataset and drawing decision boundaries between different classes.\n",
    "\n",
    "By employing these three machine learning models, we aim to harness their unique strengths and capabilities in order to accurately predict forest cover types for our dataset. Through rigorous model evaluation and comparison, we will select the best-performing model that achieves the highest prediction accuracy and generalization performance.\n",
    "\n",
    "* Decision Tree\n",
    "* Logistic Regression\n",
    "* Neural Network\n",
    "* GBoosting\n",
    "* Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67560610",
   "metadata": {},
   "source": [
    "### Definining Features and Target Variables for our models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bdff45",
   "metadata": {},
   "source": [
    "In ML, having a large amount of data is crucial for training models that can accurately make predictions. However, processing large datasets can be computationally expensive and time-consuming, especially when running multiple models. Therefore, using a smaller subset of the data can help to reduce the time and computational resources required for training and testing models.\n",
    "\n",
    "In this case, using a subset of 100,000 instances allows us to build and test multiple models quickly without sacrificing too much data. By randomly selecting a subset, we can ensure that the sample is representative of the larger dataset while reducing the overall size of the data. This approach can be particularly useful when dealing with big data, where even a small subset can be sufficiently representative of the entire dataset.\n",
    "\n",
    "Moreover, using a smaller subset can also help to avoid overfitting, which occurs when a model is trained too closely on the training data and fails to generalize to new data. By using a smaller subset, we can limit the complexity of the models we build, making them less likely to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff39d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a random subset of the data (because the dataset is too large)\n",
    "subset_size = 100000 # Change this value to the desired subset size\n",
    "random_indices = np.random.choice(df.index, subset_size, replace=False)\n",
    "df_subset = df.loc[random_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49422dd",
   "metadata": {},
   "source": [
    "The input data consists of five features: \"Wilderness area code\", \"Elevation\", \"Horizontal_Distance_To_Roadways\", \"Horizontal_Distance_To_Fire_Points\", and \"Soil type code\". These features are used to predict the cover type, which is a categorical variable represented by the \"Cover_Type_code\" column.\n",
    "\n",
    "**EXPLAIN WHY WE CHOSE THOSE FEATURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5557bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "\n",
    "#X = df_subset.drop(\"Cover_Type_code\", axis=1)\n",
    "X = df_subset[[\"Wilderness area code\", \"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Soil type code']]\n",
    "y = df_subset[\"Cover_Type_code\"]\n",
    "#X = df[[\"Wilderness area code\", \"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Soil type code']]\n",
    "#y = df[\"Cover_Type_code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c90404",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5669a7",
   "metadata": {},
   "source": [
    "Dividing the dataset into training and testing sets is an essential step in machine learning to evaluate the performance of the model. In this case, the dataset is split into 80% training data and 20% testing data.\n",
    "\n",
    "The reason for using an 80/20 split is that it strikes a balance between having enough data to train the model while still having a sufficient amount of data to evaluate the model's performance. The training data is used to fit the model parameters, while the testing data is used to evaluate how well the model generalizes to new, unseen data.\n",
    "\n",
    "If we use too little data for training, the model may not learn the underlying patterns in the data correctly, resulting in poor performance. Conversely, if we use too much data for training, the model may overfit, which means it will perform well on the training data but poorly on the testing data.\n",
    "\n",
    "The 80/20 split is a common default choice for many machine learning applications, but it can be adjusted based on the size and complexity of the dataset and the problem being solved. For example, if the dataset is small, we may need to use a higher percentage of the data for training, such as a 70/30 split. In contrast, if the dataset is large, we may use a smaller percentage for training, such as a 90/10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42cef542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dfset into training and testing subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33f9df",
   "metadata": {},
   "source": [
    "### Why Accuracy Evaluation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658c4b87",
   "metadata": {},
   "source": [
    "Accuracy is a commonly used evaluation metric in classification problems because it is easy to understand and interpret. It measures the proportion of correctly classified instances out of the total instances in the dataset. In your project, accuracy was used to evaluate the performance of the machine learning models for classifying forest cover types.\n",
    "\n",
    "However, accuracy might not always be the best evaluation metric, especially when dealing with imbalanced datasets, where some classes have a significantly higher number of instances compared to others. In such cases, a high accuracy could be misleading, as the model might simply be good at classifying the majority class while performing poorly on minority classes.\n",
    "\n",
    "There are other evaluation metrics that can provide a more comprehensive view of the model's performance, such as:\n",
    "\n",
    "- Precision: The proportion of true positive instances out of the instances predicted as positive. It measures the model's ability to correctly identify positive instances.\n",
    "- Recall: The proportion of true positive instances out of the actual positive instances. It measures the model's ability to identify all the positive instances.\n",
    "- F1 Score: The harmonic mean of precision and recall. It provides a balanced measure between precision and recall, and is especially useful when dealing with imbalanced datasets.\n",
    "- Confusion Matrix: A table that shows the true positive, false positive, true negative, and false negative predictions of a classifier. It helps to identify the types of errors the model is making.\n",
    "- Area Under the Receiver Operating Characteristic (ROC) Curve (AUC-ROC): A plot that illustrates the true positive rate (sensitivity) against the false positive rate (1-specificity) for different threshold values. AUC-ROC is a measure of how well the model can distinguish between classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc56c3cd",
   "metadata": {},
   "source": [
    "<a name=\"model1\"></a>\n",
    "\n",
    "## 1. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba143d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define features\n",
    "features = list(X.columns)\n",
    "\n",
    "# Train a decision tree classifier\n",
    "clf = tree.DecisionTreeClassifier(max_depth=4, random_state = 42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the testing df\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate the confusion matrix of the model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "class_names=[0,1,2,3,4,5,6] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(30, 20))\n",
    "plot_tree(clf, feature_names= features, class_names=['0','1', '2','3','4','5','6'], filled=True)\n",
    "plt.show()\n",
    "\n",
    "# Print the evaluation matrix\n",
    "target_names = ['0','1', '2','3','4','5','6']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227c20b",
   "metadata": {},
   "source": [
    "For now we will remove max deph to improve the aacuracy of our decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16146dd",
   "metadata": {},
   "source": [
    "<a name=\"model2\"></a>\n",
    "# 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01950c5a",
   "metadata": {},
   "source": [
    "We decided to use a logistic regression model for this task due to its simplicity, interpretability, and effectiveness in many classification problems. We performed feature scaling with StandardScaler by removing the mean and scaling to unit variance to standardize the input features due to scaling sensitivity. This way, we ensure that all features have the same scale, which helps the logistic regression algorithm converge faster and provide better performance.\n",
    "\n",
    "After feature scaling, we proceeded to train the logistic regression model using the LogisticRegression class from the scikit-learn library. We set the maximum number of iterations to 1000 to give the optimization algorithm enough iterations to converge to an optimal solution. In some cases, logistic regression models may require more iterations to find the optimal weights, especially when dealing with complex datasets or large feature spaces. By setting a higher number of iterations, we can ensure that the optimization algorithm has sufficient opportunities to find the best solution within the given constraint.\n",
    "\n",
    "We used the 'lbfgs' solver as it is a popular optimization algorithm for logistic regression. This algorithm is an efficient optimization technique that approximates the BFGS algorithm using a limited amount of memory. This solver is suitable for large-scale problems and can handle a wide range of data sizes and complexities. Additionally, the 'lbfgs' solver works well with L2 regularization, which is commonly used in logistic regression to prevent overfitting and improve generalization.\n",
    "\n",
    "We specified the multi_class parameter as 'multinomial' to handle the multi-class classification problem. In a multi-class classification problem, there are more than two classes to predict. The 'multinomial' option enables the logistic regression model to handle such problems by using the softmax function to estimate probabilities for each class. The softmax function is a generalization of the logistic function that can handle multiple classes, converting the model's output into class probabilities. By using the 'multinomial' option, we ensure that our logistic regression model is well-suited to handle the multi-class nature of the cover type classification problem.\n",
    "\n",
    "Once the logistic regression model was trained, we made predictions on the test set and evaluated its performance using accuracy as the performance metric. Accuracy is a common metric used to measure the proportion of correct predictions made by the model out of the total number of predictions. We also performed cross-validation to assess the model's generalization capability. Cross-validation is a technique used to estimate the performance of a model on unseen data by splitting the training data into multiple subsets, training the model on each subset, and evaluating the performance on the remaining data. In our case, we used 5-fold cross-validation, which means that the training data was split into five equal parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d475cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial')\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(logreg, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {np.mean(cv_scores):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db996745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(logreg, X_test_scaled, y_test, cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da0255",
   "metadata": {},
   "source": [
    "ADD CONFUSION MATRIX EXPLANATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2817e671",
   "metadata": {},
   "source": [
    "We also performed hyperparameter tuning to find the best parameters for our model. However, this didn't improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb39b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for data scaling and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000, multi_class='multinomial'))\n",
    "])\n",
    "\n",
    "# Set up a grid of hyperparameters for tuning\n",
    "param_grid = {\n",
    "    'logreg__solver': ['lbfgs', 'sag', 'saga'],\n",
    "    'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and evaluate it on the testing set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(best_model)\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df034c4c",
   "metadata": {},
   "source": [
    "We chose logistic regression with feature scaling for this cover type classification task due to its simplicity, interpretability, and effectiveness for such problems. We also evaluated the model using accuracy and cross-validation to ensure its generalizability to unseen data.\n",
    "\n",
    "While logistic regression is a simple and interpretable model often used for classification problems, it may not always be the best choice depending on the complexity and nature of the problem. In our case, the reported accuracy of 0.71 indicates that the model's performance is not optimal compared to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab9d6c",
   "metadata": {},
   "source": [
    "<a name=\"model3\"></a>\n",
    "# 3. Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d039d4",
   "metadata": {},
   "source": [
    "We used a simple feedforward neural network (also known as a multilayer perceptron) for predicting cover types, which is a classification problem. The model is implemented using the Keras library. \n",
    "\n",
    "We used the StandardScaler from scikit-learn to normalize the features. This is important as it helps to ensure that all features have a similar scale, which can improve the performance of the model.\n",
    "\n",
    "\n",
    "We ensured that the target variable is integer-encoded and starts from 0, which is required when using 'sparse_categorical_crossentropy' as the loss function. The model consists of an input layer, two hidden layers with 10 neurons each, and an output layer. The activation functions used are ReLU (rectified linear unit) for the hidden layers and softmax for the output layer. Softmax is used for the output layer because it is suitable for multi-class classification problems, as it converts the output into probabilities that sum to 1.\n",
    "\n",
    "The model is compiled using 'sparse_categorical_crossentropy' as the loss function and the 'adam' optimizer. The 'sparse_categorical_crossentropy' loss function is suitable for multi-class classification problems with integer-encoded labels. We trained the model for 50 epochs with a batch size of 10.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "473d794e",
   "metadata": {},
   "source": [
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure the target variable is integer-encoded (starting from 0)\n",
    "y_train = y_train.astype(int) - 1\n",
    "y_test = y_test.astype(int) - 1\n",
    "\n",
    "# Define the model\n",
    "num_classes = len(np.unique(y_train))\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))  # Changed activation to 'softmax' and the number of units to match the number of classes\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # Changed loss to 'sparse_categorical_crossentropy'\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_r = np.argmax(y_pred, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_r)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a09104",
   "metadata": {},
   "source": [
    "CAN ONLY BE RUN ON GOOGLE COLAB\n",
    "TAKES 1O MIN TO RUN AND GIVES AN ACCRACY OF **0.73965**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de9ea30",
   "metadata": {},
   "source": [
    "The achieved accuracy of 0.73 might not be optimal for some applications, and other models might perform better. Some reasons to consider other models include that the chosen model is relatively simple and may not capture the complexity of the relationship between the features and the target variable.\n",
    "\n",
    "Moreover, the number of layers, neurons, and activation functions were selected arbitrarily, and a more systematic approach (e.g., using a grid search or randomized search) might lead to better model configurations.\n",
    "\n",
    "Other models, such as decision trees, random forests, support vector machines, or gradient boosting machines, might have better performance on this particular problem. Ensemble methods, which combine multiple models, can also lead to improved accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de29ea",
   "metadata": {},
   "source": [
    "<a name=\"model4\"></a>\n",
    "# 4. Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af777f5f",
   "metadata": {},
   "source": [
    "<a name=\"model5\"></a>\n",
    "# 5. XG Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Choose a random subset of the data\n",
    "subset_size = 100000 # Change this value to the desired subset size\n",
    "random_indices = np.random.choice(df.index, subset_size, replace=False)\n",
    "df_subset = df.loc[random_indices]\n",
    "\n",
    "# Separate features and target variable\n",
    "\n",
    "X = df_subset.drop(\"Cover_Type_code\", axis=1)\n",
    "#X = df_subset[[\"Wilderness area code\", \"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Soil type code']]\n",
    "y = df_subset[\"Cover_Type_code\"]\n",
    "#X = df[[\"Wilderness area code\", \"Elevation\", 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points', 'Soil type code']]\n",
    "#y = df[\"Cover_Type_code\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train a Gradient Boosting model\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the testing set\n",
    "y_pred_gb = gb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "print(f\"Accuracy (Gradient Boosting): {accuracy_gb:.2f}\")\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores_gb = cross_val_score(gb_model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy (Gradient Boosting): {np.mean(cv_scores_gb):.2f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "print(\"Confusion Matrix (Gradient Boosting):\\n\", cm_gb)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(gb_model, X_test_scaled, y_test, cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix (Gradient Boosting)\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "report_gb = classification_report(y_test, y_pred_gb)\n",
    "print(\"Classification Report (Gradient Boosting):\\n\", report_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d75ea2",
   "metadata": {},
   "source": [
    "<a name=\"model6\"></a>\n",
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833c4754",
   "metadata": {},
   "source": [
    "An ensemble model combines multiple individual models to improve the overall performance. One common ensemble technique is the Voting Classifier, which takes the majority vote of different classifiers to make the final prediction. Here's a Voting Classifier using scikit-learn with a Random Forest, a Support Vector Machine, and a Gradient Boosting Classifier.\n",
    "\n",
    "In this example, we create a Voting Classifier that combines a Random Forest, a Support Vector Machine (with a radial kernel), and a Gradient Boosting Classifier. \n",
    "\n",
    "The voting parameter is set to 'soft', which means that the classifier will predict the class label based on the argmax of the class probabilities, calculated as the average of the predicted probabilities of the individual classifiers.\n",
    "\n",
    "It is possible to experiment with different combinations of classifiers and their hyperparameters to find the best performing ensemble for your problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the individual classifiers\n",
    "clf1 = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "clf2 = make_pipeline(StandardScaler(), SVC(kernel='rbf', probability=True, random_state=42))\n",
    "clf3 = GradientBoostingClassifier(n_estimators=50, random_state=42, n_iter_no_change=5, tol=0.01)\n",
    "\n",
    "# Create the ensemble model (Voting Classifier)\n",
    "eclf = VotingClassifier(estimators=[('rf', clf1), ('svc', clf2), ('gb', clf3)], voting='soft')\n",
    "\n",
    "# Train the ensemble model\n",
    "eclf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = eclf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae14c2",
   "metadata": {},
   "source": [
    "To optimize the model we can perform hyperparameter tuning to find the best parameters for the model. We used Grid Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dfc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the individual classifiers with placeholder hyperparameters\n",
    "clf1 = RandomForestClassifier(random_state=42)\n",
    "clf2 = make_pipeline(StandardScaler(), SVC(kernel='linear', probability=True, random_state=42))\n",
    "clf3 = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Create the ensemble model (Voting Classifier)\n",
    "eclf = VotingClassifier(estimators=[('rf', clf1), ('svc', clf2), ('gb', clf3)], voting='soft')\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [50, 100, 200],\n",
    "    'svc__svc__C': [0.1, 1, 10],\n",
    "    'gb__n_estimators': [50, 100, 200],\n",
    "    'gb__learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", grid.best_params_)\n",
    "\n",
    "# Make predictions on the test set using the best ensemble model found\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c053ce39",
   "metadata": {},
   "source": [
    "<a name=\"compare\"></a>\n",
    "## Model Comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4586c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "414332df",
   "metadata": {},
   "source": [
    "<a name=\"best\"></a>\n",
    "## Best Model Selection and Improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ba77f",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "## Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ddd1ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e36f5ea",
   "metadata": {},
   "source": [
    "<a name=\"sources\"></a>\n",
    "## Sources\n",
    "\n",
    "Original Owners of Database: \n",
    "\n",
    "Remote Sensing and GIS Program \n",
    "\n",
    "Department of Forest Sciences\n",
    "\n",
    "College of Natural Resources\n",
    "\n",
    "Colorado State University \n",
    "\n",
    "Fort Collins, CO 80523 \n",
    "\n",
    "(contact Jock A. Blackard, jblackard@fs.fed.us or Dr. Denis J. Dean, denis.dean@utdallas.edu) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
